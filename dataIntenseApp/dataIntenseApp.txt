+ Describing Performance: percentiles, tail latencies, head-of-line blocking
+ Coping with Load: scaling up (vertical scaling), scaling out(horizontal scaling), shared-nothing architecture, elastic

Reliability:
+ Tolerating hardware & software fault, human error
Scalability
+ Measuring load & performance
+ Latency percentiles, through put
Maintainability
+ Operability, Simplicity, Evolvability

Chapter 2: Data Models and Query Languages
+ Relational Model, Document Model, NoSQL
+ ORM, Many to One, Many to Many
+ Schema-on-read, schema-on-write
+ Declarative query, Map-Reduce
+ Graph Model: Property Graphs, Cipher Query, Triple-Store and SPARQL,

Chapter 3: Storage and Retrieval
+ log-structured storage engines, page-oriented storage engines (B-trees)
+ Hash indexes (compaction), SSTables and LSM-Trees
* SSTables: 
+ merge rapldly
+ spare index
+ compress group to write
+ in memory balanced-tree (meltable)
+ keep separate log files (immediately appended) in disk

* LSM-Trees: (Log-Structured Merge Tree)
Performance optimizations: bloom filter, sized-tiered and leveled compaction, 

* B-Tree: 
+ Branching factor
+ Write-ahead log (WAL, redo log), latches (lightweight locks)
+ Optimizations: Copy-on-write, abbreviating keys, pointer to sibling, fractal tree

* B-Tree vs LSM-Trees
+ LSM-Trees: write faster, lower write amplification, better compressed, less fragment, lower overhead
+ B-Tree: not need compaction, keys in exactly one place.

* Other Indexing Structures:
+ positing list => key, row identifier => key
+ heap file, clustered index, covering index
+ concatenated index, R-tree, 2D index,
+ in-memory databases: memcached, memsql, redis, couchbase, v..v..., anti-caching,non-volatile memory.

* OLTP (Transcation) vs OLAP (Analytics)
+ OLAP <= Data Warehouse <= Extract-Transform-Load <= OLTP
+ Star Schema, dimension tables; snowflake schema

* Column-Oriented Storage
+ Column Compression: bitmap encoding, vectorized processing, sort order
+ Writing: use LSM-Trees, materialized view

Chapter 4: Encoding and Evolution
Application changes => data changes => code changes:
+ Backward compatibility (not hard)
+ Forward compatibility (tricky)
+ Rolling update

* Format:
+ in-memory(pointers, data structures) => in-network (byte sequence): encoding
+ in-network => in-memory: decoding

1) XML, JSON
- Problems: 
+ Too large number for IEEE754 double-precision (Javascript)
+ Don't support binary strings, if use Base64 -> data size increases 33%
- Using for interchange

2) Binary encoding
- compact, faster => using for internally
- Thrift (facebook), Protocol Buffers(google) : has a schema definition
- Avro (Hadoop):
+ Schema: Avro IDL, JSON
+ Dynamically generated schemas, code
+ 
- The Merits of Schemas: compact, documentation, check compatibility, generate code

3) Modes of Dataflow:
- Through Databases: 
- Through Services:
+ REST => public API
+ RPC: complex, fundamentally different with local function, service discovery => between services

4) Message-Passing Dataflow
+ message queue(broker), distributed actor (Akka, Orleans, Erlang OTP)
